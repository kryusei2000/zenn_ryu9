---
title: "k-means++ についてまとめてみた"
emoji: "💎"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["kmeanspp", "kmeans", "クラスタリング", "機械学習"]
published: false
---

## この記事で学べること
みなさん、こんにちは。りゅう9です。この記事では、クラスタリングの代表格、**k-means法 (k平均法)** を使うときに役立つ**k-means＋＋**について解説します。

## そもそもk-means法 (k平均法) とは?
クラスタリングとは「データを似ているもの同士でグループ分けする」手法のことです。
その代表が**k-means法**です。

k-means法では以下のような手法でクラスタリングを行います。
1. **初期化**
   まずは、データの中から設定したクラスタ数分だけランダムに点を選びます。この点を**クラスタ中心 (重心)** とします。
2. **データの割り当て**
   各データ点を、最も近いクラスタ中心に所属させます。
3. **クラスタ中心の再計算**
   各クラスタに所属するデータの平均位置 (重心) を計算し、その位置を新しいクラスタ中心とします。
4. **繰り返し**
   クラスタ中心が変わらなくなる (もしくは、変化が設定した閾値以下になる、データの割り当てが変化しなくなる、設定した繰り返し回数に到達する) まで2と3を繰り返します。

詳しくはこちらの記事をご覧ください。
https://zenn.dev/ryu9/articles/clustering-kmeans

:::message
しかし、この方法では問題があります。
それは、初期化で設定した点の位置が悪いと、クラスタリングの精度が低くなったり、収束が遅くなったり（繰り返し回数が多くなったり）します。
**そこで登場するのがk-means++です。**
:::

## k-means++とは? ざっくりとした流れ
k-means++はk-means法の初期化の部分に工夫を加えた手法です。通常のk-means法では「ランダム」に初期値を選んでいたのに対して、k-means++では以下の手法で初期中心を選びます。

1. データ点からランダムに1点を選び、最初の中心とする。
2. 残りのデータ点に対して、1. の点 (既存の中心) との距離の2乗を計算する。
3. 先ほどの2. の距離の2乗を使って確率を計算する。（この式は後ほど解説します。）
   この確率は、既存の中心から離れているほど選ばれやすいように設定します。
4. k個 (指定したクラスタ数分) の中心が選ばれるまで2. と3. を繰り返す。
5. 中心を選び終えたら通常のk-means法の手法でクラスタリングをする。