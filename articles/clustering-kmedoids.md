---
title: "クラスタリングの手法の1つ　k-medoids法についてまとめてみた"
emoji: "🐕"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["kmedoids", "クラスタリング", "機械学習"]
published: false
---

## この記事で学べること
みなさん、こんにちは。りゅう9です。この記事では、クラスタリングの手法の1つ、**k-medoids法** についてそのアルゴリズムを解説します。クラスタリングの代表と言えば**k-means法 (k平均法)** ですが、このk-means法との違いを含めて解説します。プログラムを用いた実装はやらずに、アルゴリズムの理解をメインとします。

## この記事の想定読者
この記事ではk-means法 (k平均法) について理解していることを前提で話を進めます。[こちら](https://zenn.dev/ryu9/articles/clustering-kmeans)の記事でk-means法について解説しているので参照してください。

## k-medoids法とは k-means法との違いも含めて
k-medoids法はクラスタリング手法の1つで、k-means法と同じ非階層型クラスタリングに分類されます。非階層型クラスタリングの多くの手法ではあらかじめクラスタ数を決める必要があります。[^1]「クラスタ数を決める」というのは、データをいくつのグループに分けるかを決めるということです。データをいくつに分けるかをあらかじめ決めて、データをその数に分けます。k-means法では各クラスタの「中心（重心、centroid）」を基準にデータを分類するのに対し、k-medoids法では**各クラスタの中の実データの1つを中心にして**データを分類します。つまり、k-medoids法では**実データ**がクラスタ中心となり、k-means法では重心で計算された仮想的な点 (実データではないことがほとんど) がクラスタ中心となります。k-means法でのクラスタ中心のことを**セントロイド (centroid)** というのに対し、k-medoids法ではクラスタ中心のことを**メドイド (medoid)** と言います。イメージをFig. 1に示します。(実際のメドイド、セントロイドではないことに注意して下さい)
![k-medoids法のイメージ](/images/20250816_fig1.png)
*Fig. 1 k-medoids法とk-means法のイメージ*

## ざっくりとしたk-medoids法の流れ
0. **距離・非類似度の定義**
   各データ点とメドイドの距離 (非類似度) を定義します。k-means法ではユークリッド距離を使うことが多いですが、k-medoids法では様々な距離関数を使用することができます。
1. **初期化**
   まずは、データの中から設定したクラスタ数分だけランダムに点を選びます。この点を**クラスタ中心 (メドイド)** とします。
2. **データの割り当て**
   各データ点を、最も近いメドイドに所属させます。
3. **メドイドの更新**
   各クラスタについて、ほかの点をメドイドとしてコスト (クラスタ内の全データ点と中心の距離の合計) を計算します。コストが小さくなるならば、メドイドをその点に入れ替えます。
4. **繰り返し**
   メドイドが変わらなくなるまで、2と3を繰り返します。

**基本的な流れはk-means法と変わらないことがわかったでしょうか?**

## 各ステップの紹介
それでは、各ステップの詳細を説明します。
まずは、下のFig. 1をご覧ください。今回は、xy平面上にランダムに打った40点のデータを3つのクラスタにクラスタリングしてみます。~~ランダムに打ったことが原因でクラスタリングの例としてはわかりにくいデータになってしまいました。すみません。~~
![初期データ]( =330x)
*Fig. 1 初期データ*
1. **初期化**
   データの中からランダムに指定したクラスタの数だけ点を選んでクラスタ中心 (**メドイド**) とします。(今回は3)  
   Fig. 2ではランダムに選んだ点を★として描画しています。  

2. **データの割り当て**
3. **メドイドの更新**
4. **繰り返し**


## k-medoids法のメリット・デメリット
k-medoids法のメリットとデメリットをいくつか記します。

### メリット
- **外れ値に強い**
  k-meansでは外れ値があるとそれに引っ張られて中心がずれることがありますが、k-medoidsでは実際のデータ点をクラスタ中心に選ぶので外れ値に強くなります。
- **クラスタ中心が実際のデータ点である**
  結果として得られるクラスタ中心 (メドイド) が実際のデータ点なので、代表値として使いやすいです。
- **任意の距離関数を使用できる**
  ユークリッド距離だけではなく、マンハッタン距離やコサイン距離などでもクラスタリングできます。それ以外にも様々な距離関数を利用することができ、点だけではなく波形のクラスタリングなどもできます。

### デメリット
- **計算コストが高い**
  メドイド更新時にクラスタ内のすべての点間距離を計算するため、計算コストが高くなります。
- **大規模データには不向き**
  計算コストが高いため、データ数の多い大規模データでは距離が非常に重くなります。
- **局所最適解に陥ることがある**
  初期メドイドの選び方によっては最終結果が異なる可能性があり、局所最適解に陥ることがあります。複数回試行して、評価値 (シルエットスコアやSSEなど) が最もよくなる結果を採用します。

## まとめ

[^1]:[クラスタリングとは? ~各手法説明のための導入として~](https://zenn.dev/ryu9/articles/what_clastering#%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%AE%E7%A8%AE%E9%A1%9E)